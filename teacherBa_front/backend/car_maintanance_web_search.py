# -*- coding: utf-8 -*-
"""Car_Maintanance_web_search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dQ87xTcuVpOzHwr8o6GAvapTxMbIaJws

# ì°¨ëŸ‰ì •ë¹„
- RAG(PDF)
- LLM
"""

# !pip install openai
# !pip install python-dotenv
# !pip install langchain-openai
# !pip install langchain_community
# !pip install pypdf
# !pip install chromadb
# !pip install langchain_chroma

import os
import openai
from IPython.display import Markdown, display

from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings

from langchain_community.utils.math import cosine_similarity
from langchain_core.prompts import ChatPromptTemplate
import numpy as np
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

from langchain_community.document_loaders import PyPDFLoader

# PDF ë¡œë” ì´ˆê¸°í™” (ê·¼ë¡œê¸°ì¤€ë²• ë¬¸ì„œ)
pdf_loader_1 = PyPDFLoader("/content/Crawfords_Auto_Repair_Guide.pdf")
pdf_loader_2 = PyPDFLoader("/content/Ultimate-Guide-to-Vehicle-Maintenance.pdf")

# ë™ê¸° ë¡œë”©
pdf_docs_1 = pdf_loader_1.load()
pdf_docs_2 = pdf_loader_2.load()

print("ë¬¸ì„œì˜ ìˆ˜ :", len(pdf_docs_1))
print("-" * 100)
print("ì²˜ìŒ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° :", pdf_docs_1[0].metadata)

print("*" * 100)
print("ë¬¸ì„œì˜ ìˆ˜ :", len(pdf_docs_2))
print("-" * 100)
print("ì²˜ìŒ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° :", pdf_docs_2[0].metadata)

print("ì²« ë²ˆì§¸ ë¬¸ì„œ : ", pdf_docs_1[3].page_content)
print("-" * 130)
print("ì²« ë²ˆì§¸ ë¬¸ì„œ : ", pdf_docs_2[3].page_content)

"""### Text splitter - pdf_docs_1"""

# ì¬ê·€ì  í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,             # ì²­í¬ í¬ê¸°
    chunk_overlap=200,           # ì²­í¬ ì¤‘ ì¤‘ë³µë˜ëŠ” ë¶€ë¶„ í¬ê¸°
    length_function=len,         # ê¸€ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    separators=["\n\n", "\n", " ", ""],  # êµ¬ë¶„ì - ì¬ê·€ì ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì ìš© [ë¬¸ë‹¨, ë¬¸ì¥, ë„ì–´ì“°ê¸°, ê³µë°±]
)

# split_documents() ë©”ì„œë“œ ì‚¬ìš© : Document ê°ì²´ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ ì²­í¬ ë¬¸ì„œë¡œ ë¶„í• 
chunks = text_splitter.split_documents(pdf_docs_1)
print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜: {len(chunks)}")
print(f"ê° ì²­í¬ì˜ ê¸¸ì´: {list(len(chunk.page_content) for chunk in chunks)}")
print()

# # ê° ì²­í¬ì˜ ì‹œì‘ ë¶€ë¶„ê³¼ ë ë¶€ë¶„ í™•ì¸ - 5ê°œ ì²­í¬ë§Œ ì¶œë ¥
# for chunk in chunks[:5]:
#     print(chunk.page_content[:200])
#     print("-" * 100)
#     print(chunk.page_content[-200:])
#     print("=" * 100)
#     print()

"""### Text splitter - pdf_docs_2"""

# ì¬ê·€ì  í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,             # ì²­í¬ í¬ê¸°
    chunk_overlap=200,           # ì²­í¬ ì¤‘ ì¤‘ë³µë˜ëŠ” ë¶€ë¶„ í¬ê¸°
    length_function=len,         # ê¸€ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    separators=["\n\n", "\n", " ", ""],  # êµ¬ë¶„ì - ì¬ê·€ì ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì ìš© [ë¬¸ë‹¨, ë¬¸ì¥, ë„ì–´ì“°ê¸°, ê³µë°±]
)

# split_documents() ë©”ì„œë“œ ì‚¬ìš© : Document ê°ì²´ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ ì²­í¬ ë¬¸ì„œë¡œ ë¶„í• 
chunks = text_splitter.split_documents(pdf_docs_2)
print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜: {len(chunks)}")
print(f"ê° ì²­í¬ì˜ ê¸¸ì´: {list(len(chunk.page_content) for chunk in chunks)}")
print()

# # ê° ì²­í¬ì˜ ì‹œì‘ ë¶€ë¶„ê³¼ ë ë¶€ë¶„ í™•ì¸ - 5ê°œ ì²­í¬ë§Œ ì¶œë ¥
# for chunk in chunks[:5]:
#     print(chunk.page_content[:200])
#     print("-" * 100)
#     print(chunk.page_content[-200:])
#     print("=" * 100)
#     print()

embeddings_model = AzureOpenAIEmbeddings(model="text-embedding-3-small")

document_embeddings_openai = embeddings_model.embed_documents([doc.page_content for doc in pdf_docs_1])
print(f"ì„ë² ë”© ë°±í„°ì˜ ê°œìˆ˜: {len(document_embeddings_openai)}")
print(f"ê° ë²¡í„°ì˜ ì°¨ì›: {len(document_embeddings_openai[0])}")

len(document_embeddings_openai[0])

document_embeddings_openai = embeddings_model.embed_documents([doc.page_content for doc in pdf_docs_2])
print(f"ì„ë² ë”© ë°±í„°ì˜ ê°œìˆ˜: {len(document_embeddings_openai)}")
print(f"ê° ë²¡í„°ì˜ ì°¨ì›: {len(document_embeddings_openai[0])}")

len(document_embeddings_openai[0])

q = embeddings_model.embed_query("ì°¨ì—ì„œ ì´ìƒí•œ ëƒ„ìƒˆë‚˜ ì—°ê¸°ê°€ ë³´ì´ë©´ ì–´ë–»ê²Œ ì ê²€í•´ì•¼ í•´?")

# web searching tool
# !pip install python-dotenv

from dotenv import load_dotenv
load_dotenv()
import os
from glob import glob

from pprint import pprint
import json

from langchain_community.tools import TavilySearchResults

tool = TavilySearchResults(
             max_results=5,
             search_depth="advanced", #basic, advanced 2ê°€ì§€ê°€ ìˆë‹¤.
             include_answer=True,
             include_raw_content=True,
             include_images=True
)

# Prompt Template
from langchain_core.prompts import ChatPromptTemplate

# template = """ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹œì˜¤.
template = """

[ì§€ì¹¨]
- ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•íˆ ê·¸ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•  ê²ƒ
- ë‹µë³€ì€ ë…¼ë¦¬ì ì´ê³  êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì œê³µí•  ê²ƒ
- ë‹µë³€ì€ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•  ê²ƒ


[ì§ˆë¬¸]
{question}

[ë‹µë³€ í˜•ì‹]
ë‹µë³€: (ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€)

[ë‹µë³€]
"""

prompt = ChatPromptTemplate.from_template(template)

# í…œí”Œë¦¿ ì¶œë ¥
prompt.pretty_print()

# ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸° í•¨ìˆ˜
def find_most_similar(
        query: str,
        doc_embeddings: np.ndarray,
        embeddings_model=AzureOpenAIEmbeddings(model="text-embedding-3-small") # Removed dimensions parameter
        ) -> tuple[str, float]:

    # ì¿¼ë¦¬ ì„ë² ë”©: OpenAI ì„ë² ë”© ì‚¬ìš©
    query_embedding = embeddings_model.embed_query(query)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]

    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
    most_similar_idx = np.argmax(similarities)

    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œì™€ ìœ ì‚¬ë„ ë°˜í™˜: ë¬¸ì„œ, ìœ ì‚¬ë„
    return pdf_docs_1[most_similar_idx].page_content, similarities[most_similar_idx] # Assuming pdf_docs_1 is the correct document list

# ì˜ˆì œ ì¿¼ë¦¬
queries = [
    "ì°¨ì—ì„œ ì´ìƒí•œ ëƒ„ìƒˆë‚˜ ì—°ê¸°ê°€ ë³´ì´ë©´ ì–´ë–»ê²Œ ì ê²€í•´ì•¼ í•´?"
]

# ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
for query in queries:
    most_similar_doc, similarity = find_most_similar(query, document_embeddings_openai)
    print(f"ì¿¼ë¦¬: {query}")
    # print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ: {most_similar_doc}")
    # print(f"ìœ ì‚¬ë„: {similarity}")
    if similarity < 0.7:
        result = tool.invoke(query)
        print(rag_chain.invoke(str(result)))
        # similarity =

    else:
        print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ: {most_similar_doc}")

    print("-" * 100)



# ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸° í•¨ìˆ˜
def find_most_similar(
        query: str,
        doc_embeddings: np.ndarray,
        embeddings_model=AzureOpenAIEmbeddings(model="text-embedding-3-small") # Removed dimensions parameter
        ) -> tuple[str, float]:

    # ì¿¼ë¦¬ ì„ë² ë”©: OpenAI ì„ë² ë”© ì‚¬ìš©
    query_embedding = embeddings_model.embed_query(query)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]

    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
    most_similar_idx = np.argmax(similarities)

    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œì™€ ìœ ì‚¬ë„ ë°˜í™˜: ë¬¸ì„œ, ìœ ì‚¬ë„
    return pdf_docs_1[most_similar_idx].page_content, similarities[most_similar_idx] # Assuming pdf_docs_1 is the correct document list

# ì˜ˆì œ ì¿¼ë¦¬
queries = [
    "ì°¨ì—ì„œ ì´ìƒí•œ ëƒ„ìƒˆë‚˜ ì—°ê¸°ê°€ ë³´ì´ë©´ ì–´ë–»ê²Œ ì ê²€í•´ì•¼ í•´?"
]

# ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
for query in queries:
    most_similar_doc, similarity = find_most_similar(query, document_embeddings_openai)

    print(f"ì¿¼ë¦¬: {query}")

    if similarity < 0.7:
        result = tool.invoke(query)
        generated_answer = result["answer"]
        print("RAG ê¸°ë°˜ ìƒì„± ë‹µë³€:")
        print(generated_answer)

        # ìƒˆë¡œ ìƒì„±ëœ ë‹µë³€ì˜ ìœ ì‚¬ë„ ì¸¡ì •
        gen_answer_embedding = embeddings_model.embed_query(generated_answer)
        gen_similarity = cosine_similarity([gen_answer_embedding], document_embeddings_openai)[0]
        max_gen_similarity = max(gen_similarity)

        print(f"ğŸ” ìƒì„±ëœ ë‹µë³€ì˜ ìµœëŒ€ ìœ ì‚¬ë„: {max_gen_similarity:.4f}")

    else:
        print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ: {most_similar_doc}")
        print(f"âœ… ê¸°ì¡´ ë¬¸ì„œ ìœ ì‚¬ë„: {similarity:.4f}")

    print("-" * 100)

from langchain_community.document_loaders import PyPDFLoader

# PDF ë¡œë” ì´ˆê¸°í™” (ê·¼ë¡œê¸°ì¤€ë²• ë¬¸ì„œ)
pdf_loader = PyPDFLoader("/content/Crawfords_Auto_Repair_Guide.pdf")

# ë™ê¸° ë¡œë”©
pdf_docs = pdf_loader.load()
print("ë¬¸ì„œì˜ í˜ì´ì§€ ìˆ˜:", len(pdf_docs))

from langchain_text_splitters import RecursiveCharacterTextSplitter

# TikToken ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬ê·€ì  í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” (í† í° ìˆ˜ ê¸°ì¤€ ë¶„í• )
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base",
    chunk_size=500,
    chunk_overlap=100,
)

# split_documents() ë©”ì„œë“œ ì‚¬ìš© : Document ê°ì²´ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ ì²­í¬ ë¬¸ì„œë¡œ ë¶„í• 
chunks = text_splitter.split_documents(pdf_docs)

print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
print(f"ê° ì²­í¬ì˜ ê¸¸ì´: {list(len(chunk.page_content) for chunk in chunks)}")

# ê° ì²­í¬ì˜ ì‹œì‘ ë¶€ë¶„ê³¼ ë ë¶€ë¶„ í™•ì¸
for chunk in chunks[:3]:
    print(chunk.page_content[:100] + "---" + chunk.page_content[-100:])
    print("-" * 100)

from langchain_chroma import Chroma
from langchain_openai import AzureOpenAIEmbeddings

# OpenAI ì„ë² ë”© ëª¨ë¸ ìƒì„±
embedding_model = AzureOpenAIEmbeddings(model = "text-embedding-3-small")  # ì°¨ì› ì§€ì • ì•ˆ í•˜ë©´ 1500ì°¨ì› ë¨

# Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±í•˜ê¸°
chroma_db = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory="./chroma_db"
)

# ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ (ë²¡í„°dbì— ì €ì¥ì´ ëœ ìƒíƒœ)
chroma_db._collection.count()

from langchain_chroma import Chroma

# ì €ì¥ëœ ë²¡í„° ì €ì¥ì†Œë¥¼ ê°€ì ¸ì˜¤ê¸°
chroma_db = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embedding_model
)


results = chroma_db.similarity_search_with_score(
                  query,
                  k=3,                    #3ê°œë§Œ ë½‘ì•„ì¤˜
                  filter={"source":"/content/Crawfords_Auto_Repair_Guide.pdf"}   # íŠ¹ì • pdf ì§€ì •
)

results[0]

#prompt
template = """ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹œì˜¤.

[ì§€ì¹¨]
- ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•  ê²ƒ
- ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ
- ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° "ì£¼ì–´ì§„ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ì‘ë‹µí•  ê²ƒ
- ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•íˆ ê·¸ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•  ê²ƒ
- ë‹µë³€ì€ ë…¼ë¦¬ì ì´ê³  êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì œê³µí•  ê²ƒ
- ë‹µë³€ì€ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•  ê²ƒ

[ì»¨í…ìŠ¤íŠ¸]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€ í˜•ì‹]
1. í•µì‹¬ ë‹µë³€: (ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€)
2. ê·¼ê±°: (ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ëœ ê´€ë ¨ ì •ë³´)
3. ì¶”ê°€ ì„¤ëª…: (í•„ìš”í•œ ê²½ìš° ë¶€ì—° ì„¤ëª… ì œê³µ)

[ë‹µë³€]
"""

prompt = ChatPromptTemplate.from_template(template)

# í…œí”Œë¦¿ ì¶œë ¥
prompt.pretty_print()

#model
llm = AzureChatOpenAI(
    deployment_name="gpt-4o-mini",
    temperature=0.2)

#chain
retriever = chroma_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5,
                   "fetch_k": 10,
                   "lambda_mult": 0.3},
)


# ë¬¸ì„œ í¬ë§·íŒ…
def format_docs(docs):
    return "\n\n".join([f"{doc.page_content}" for doc in docs])

# RAG ì²´ì¸ ìƒì„±
rag_chain = (
    RunnableParallel(
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
    )
    | prompt
    | llm
    | StrOutputParser()
)

# ì²´ì¸ ì‹¤í–‰
query = "ì°¨ì—ì„œ ì´ìƒí•œ ëƒ„ìƒˆë‚˜ ì—°ê¸°ê°€ ë³´ì´ë©´ ì–´ë–»ê²Œ ì ê²€í•´ì•¼ í•´?"
output = rag_chain.invoke(query)

print(f"ì¿¼ë¦¬: {query}")
print("ë‹µë³€:")
print(output)



"""# LangGraph"""

# !pip install python-dotenv langchain_openai
# !pip install tavily-python langchain-community langchain-openai
# !pip install langgraph
# !pip install tavily-python langchain-community langchain-openai

# !pip install python-dotenv

from dotenv import load_dotenv
load_dotenv()
import os
from glob import glob

from pprint import pprint
import json

import os
from glob import glob

from pprint import pprint
import json

from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_openai import AzureChatOpenAI
from langchain_core.output_parsers import StrOutputParser



# Prompt Template
from langchain_core.prompts import ChatPromptTemplate

# template = """ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹œì˜¤.
template = """

[ì§€ì¹¨]
- ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•íˆ ê·¸ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•  ê²ƒ
- ë‹µë³€ì€ ë…¼ë¦¬ì ì´ê³  êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì œê³µí•  ê²ƒ
- ë‹µë³€ì€ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•  ê²ƒ


[ì§ˆë¬¸]
{question}

[ë‹µë³€ í˜•ì‹]
ë‹µë³€: (ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€)

[ë‹µë³€]
"""

prompt = ChatPromptTemplate.from_template(template)

# í…œí”Œë¦¿ ì¶œë ¥
prompt.pretty_print()

result = tool.invoke(query)
print(rag_chain.invoke(str(result)))

