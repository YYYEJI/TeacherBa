# -*- coding: utf-8 -*-
"""Car_Maintanance_web_search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dQ87xTcuVpOzHwr8o6GAvapTxMbIaJws

# 차량정비
- RAG(PDF)
- LLM
"""

# !pip install openai
# !pip install python-dotenv
# !pip install langchain-openai
# !pip install langchain_community
# !pip install pypdf
# !pip install chromadb
# !pip install langchain_chroma

import os
import openai
from IPython.display import Markdown, display

from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings

from langchain_community.utils.math import cosine_similarity
from langchain_core.prompts import ChatPromptTemplate
import numpy as np
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

from langchain_community.document_loaders import PyPDFLoader

# PDF 로더 초기화 (근로기준법 문서)
pdf_loader_1 = PyPDFLoader("/content/Crawfords_Auto_Repair_Guide.pdf")
pdf_loader_2 = PyPDFLoader("/content/Ultimate-Guide-to-Vehicle-Maintenance.pdf")

# 동기 로딩
pdf_docs_1 = pdf_loader_1.load()
pdf_docs_2 = pdf_loader_2.load()

print("문서의 수 :", len(pdf_docs_1))
print("-" * 100)
print("처음 문서의 메타데이터 :", pdf_docs_1[0].metadata)

print("*" * 100)
print("문서의 수 :", len(pdf_docs_2))
print("-" * 100)
print("처음 문서의 메타데이터 :", pdf_docs_2[0].metadata)

print("첫 번째 문서 : ", pdf_docs_1[3].page_content)
print("-" * 130)
print("첫 번째 문서 : ", pdf_docs_2[3].page_content)

"""### Text splitter - pdf_docs_1"""

# 재귀적 텍스트 분할기 초기화
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,             # 청크 크기
    chunk_overlap=200,           # 청크 중 중복되는 부분 크기
    length_function=len,         # 글자 수를 기준으로 분할
    separators=["\n\n", "\n", " ", ""],  # 구분자 - 재귀적으로 순차적으로 적용 [문단, 문장, 띄어쓰기, 공백]
)

# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할
chunks = text_splitter.split_documents(pdf_docs_1)
print(f"생성된 텍스트 청크 수: {len(chunks)}")
print(f"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}")
print()

# # 각 청크의 시작 부분과 끝 부분 확인 - 5개 청크만 출력
# for chunk in chunks[:5]:
#     print(chunk.page_content[:200])
#     print("-" * 100)
#     print(chunk.page_content[-200:])
#     print("=" * 100)
#     print()

"""### Text splitter - pdf_docs_2"""

# 재귀적 텍스트 분할기 초기화
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,             # 청크 크기
    chunk_overlap=200,           # 청크 중 중복되는 부분 크기
    length_function=len,         # 글자 수를 기준으로 분할
    separators=["\n\n", "\n", " ", ""],  # 구분자 - 재귀적으로 순차적으로 적용 [문단, 문장, 띄어쓰기, 공백]
)

# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할
chunks = text_splitter.split_documents(pdf_docs_2)
print(f"생성된 텍스트 청크 수: {len(chunks)}")
print(f"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}")
print()

# # 각 청크의 시작 부분과 끝 부분 확인 - 5개 청크만 출력
# for chunk in chunks[:5]:
#     print(chunk.page_content[:200])
#     print("-" * 100)
#     print(chunk.page_content[-200:])
#     print("=" * 100)
#     print()

embeddings_model = AzureOpenAIEmbeddings(model="text-embedding-3-small")

document_embeddings_openai = embeddings_model.embed_documents([doc.page_content for doc in pdf_docs_1])
print(f"임베딩 백터의 개수: {len(document_embeddings_openai)}")
print(f"각 벡터의 차원: {len(document_embeddings_openai[0])}")

len(document_embeddings_openai[0])

document_embeddings_openai = embeddings_model.embed_documents([doc.page_content for doc in pdf_docs_2])
print(f"임베딩 백터의 개수: {len(document_embeddings_openai)}")
print(f"각 벡터의 차원: {len(document_embeddings_openai[0])}")

len(document_embeddings_openai[0])

q = embeddings_model.embed_query("차에서 이상한 냄새나 연기가 보이면 어떻게 점검해야 해?")

# web searching tool
# !pip install python-dotenv

from dotenv import load_dotenv
load_dotenv()
import os
from glob import glob

from pprint import pprint
import json

from langchain_community.tools import TavilySearchResults

tool = TavilySearchResults(
             max_results=5,
             search_depth="advanced", #basic, advanced 2가지가 있다.
             include_answer=True,
             include_raw_content=True,
             include_images=True
)

# Prompt Template
from langchain_core.prompts import ChatPromptTemplate

# template = """주어진 컨텍스트를 기반으로 질문에 답변하시오.
template = """

[지침]
- 불확실한 경우 명확히 그 불확실성을 표현할 것
- 답변은 논리적이고 구조화된 형태로 제공할 것
- 답변은 한국어를 사용할 것


[질문]
{question}

[답변 형식]
답변: (질문에 대한 직접적인 답변)

[답변]
"""

prompt = ChatPromptTemplate.from_template(template)

# 템플릿 출력
prompt.pretty_print()

# 쿼리와 가장 유사한 문서 찾기 함수
def find_most_similar(
        query: str,
        doc_embeddings: np.ndarray,
        embeddings_model=AzureOpenAIEmbeddings(model="text-embedding-3-small") # Removed dimensions parameter
        ) -> tuple[str, float]:

    # 쿼리 임베딩: OpenAI 임베딩 사용
    query_embedding = embeddings_model.embed_query(query)

    # 코사인 유사도 계산
    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]

    # 가장 유사한 문서 인덱스 찾기
    most_similar_idx = np.argmax(similarities)

    # 가장 유사한 문서와 유사도 반환: 문서, 유사도
    return pdf_docs_1[most_similar_idx].page_content, similarities[most_similar_idx] # Assuming pdf_docs_1 is the correct document list

# 예제 쿼리
queries = [
    "차에서 이상한 냄새나 연기가 보이면 어떻게 점검해야 해?"
]

# 각 쿼리에 대해 가장 유사한 문서 찾기
for query in queries:
    most_similar_doc, similarity = find_most_similar(query, document_embeddings_openai)
    print(f"쿼리: {query}")
    # print(f"가장 유사한 문서: {most_similar_doc}")
    # print(f"유사도: {similarity}")
    if similarity < 0.7:
        result = tool.invoke(query)
        print(rag_chain.invoke(str(result)))
        # similarity =

    else:
        print(f"가장 유사한 문서: {most_similar_doc}")

    print("-" * 100)



# 쿼리와 가장 유사한 문서 찾기 함수
def find_most_similar(
        query: str,
        doc_embeddings: np.ndarray,
        embeddings_model=AzureOpenAIEmbeddings(model="text-embedding-3-small") # Removed dimensions parameter
        ) -> tuple[str, float]:

    # 쿼리 임베딩: OpenAI 임베딩 사용
    query_embedding = embeddings_model.embed_query(query)

    # 코사인 유사도 계산
    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]

    # 가장 유사한 문서 인덱스 찾기
    most_similar_idx = np.argmax(similarities)

    # 가장 유사한 문서와 유사도 반환: 문서, 유사도
    return pdf_docs_1[most_similar_idx].page_content, similarities[most_similar_idx] # Assuming pdf_docs_1 is the correct document list

# 예제 쿼리
queries = [
    "차에서 이상한 냄새나 연기가 보이면 어떻게 점검해야 해?"
]

# 각 쿼리에 대해 가장 유사한 문서 찾기
for query in queries:
    most_similar_doc, similarity = find_most_similar(query, document_embeddings_openai)

    print(f"쿼리: {query}")

    if similarity < 0.7:
        result = tool.invoke(query)
        generated_answer = result["answer"]
        print("RAG 기반 생성 답변:")
        print(generated_answer)

        # 새로 생성된 답변의 유사도 측정
        gen_answer_embedding = embeddings_model.embed_query(generated_answer)
        gen_similarity = cosine_similarity([gen_answer_embedding], document_embeddings_openai)[0]
        max_gen_similarity = max(gen_similarity)

        print(f"🔍 생성된 답변의 최대 유사도: {max_gen_similarity:.4f}")

    else:
        print(f"가장 유사한 문서: {most_similar_doc}")
        print(f"✅ 기존 문서 유사도: {similarity:.4f}")

    print("-" * 100)

from langchain_community.document_loaders import PyPDFLoader

# PDF 로더 초기화 (근로기준법 문서)
pdf_loader = PyPDFLoader("/content/Crawfords_Auto_Repair_Guide.pdf")

# 동기 로딩
pdf_docs = pdf_loader.load()
print("문서의 페이지 수:", len(pdf_docs))

from langchain_text_splitters import RecursiveCharacterTextSplitter

# TikToken 인코더를 사용하여 재귀적 텍스트 분할기 초기화 (토큰 수 기준 분할)
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base",
    chunk_size=500,
    chunk_overlap=100,
)

# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할
chunks = text_splitter.split_documents(pdf_docs)

print(f"생성된 청크 수: {len(chunks)}")
print(f"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}")

# 각 청크의 시작 부분과 끝 부분 확인
for chunk in chunks[:3]:
    print(chunk.page_content[:100] + "---" + chunk.page_content[-100:])
    print("-" * 100)

from langchain_chroma import Chroma
from langchain_openai import AzureOpenAIEmbeddings

# OpenAI 임베딩 모델 생성
embedding_model = AzureOpenAIEmbeddings(model = "text-embedding-3-small")  # 차원 지정 안 하면 1500차원 됨

# Chroma 벡터 저장소 생성하기
chroma_db = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory="./chroma_db"
)

# 문서 개수 확인 (벡터db에 저장이 된 상태)
chroma_db._collection.count()

from langchain_chroma import Chroma

# 저장된 벡터 저장소를 가져오기
chroma_db = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embedding_model
)


results = chroma_db.similarity_search_with_score(
                  query,
                  k=3,                    #3개만 뽑아줘
                  filter={"source":"/content/Crawfords_Auto_Repair_Guide.pdf"}   # 특정 pdf 지정
)

results[0]

#prompt
template = """주어진 컨텍스트를 기반으로 질문에 답변하시오.

[지침]
- 컨텍스트에 있는 정보만을 사용하여 답변할 것
- 외부 지식이나 정보를 사용하지 말 것
- 컨텍스트에서 답을 찾을 수 없는 경우 "주어진 정보만으로는 답변하기 어렵습니다."라고 응답할 것
- 불확실한 경우 명확히 그 불확실성을 표현할 것
- 답변은 논리적이고 구조화된 형태로 제공할 것
- 답변은 한국어를 사용할 것

[컨텍스트]
{context}

[질문]
{question}

[답변 형식]
1. 핵심 답변: (질문에 대한 직접적인 답변)
2. 근거: (컨텍스트에서 발견된 관련 정보)
3. 추가 설명: (필요한 경우 부연 설명 제공)

[답변]
"""

prompt = ChatPromptTemplate.from_template(template)

# 템플릿 출력
prompt.pretty_print()

#model
llm = AzureChatOpenAI(
    deployment_name="gpt-4o-mini",
    temperature=0.2)

#chain
retriever = chroma_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5,
                   "fetch_k": 10,
                   "lambda_mult": 0.3},
)


# 문서 포맷팅
def format_docs(docs):
    return "\n\n".join([f"{doc.page_content}" for doc in docs])

# RAG 체인 생성
rag_chain = (
    RunnableParallel(
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
    )
    | prompt
    | llm
    | StrOutputParser()
)

# 체인 실행
query = "차에서 이상한 냄새나 연기가 보이면 어떻게 점검해야 해?"
output = rag_chain.invoke(query)

print(f"쿼리: {query}")
print("답변:")
print(output)



"""# LangGraph"""

# !pip install python-dotenv langchain_openai
# !pip install tavily-python langchain-community langchain-openai
# !pip install langgraph
# !pip install tavily-python langchain-community langchain-openai

# !pip install python-dotenv

from dotenv import load_dotenv
load_dotenv()
import os
from glob import glob

from pprint import pprint
import json

import os
from glob import glob

from pprint import pprint
import json

from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_openai import AzureChatOpenAI
from langchain_core.output_parsers import StrOutputParser



# Prompt Template
from langchain_core.prompts import ChatPromptTemplate

# template = """주어진 컨텍스트를 기반으로 질문에 답변하시오.
template = """

[지침]
- 불확실한 경우 명확히 그 불확실성을 표현할 것
- 답변은 논리적이고 구조화된 형태로 제공할 것
- 답변은 한국어를 사용할 것


[질문]
{question}

[답변 형식]
답변: (질문에 대한 직접적인 답변)

[답변]
"""

prompt = ChatPromptTemplate.from_template(template)

# 템플릿 출력
prompt.pretty_print()

result = tool.invoke(query)
print(rag_chain.invoke(str(result)))

