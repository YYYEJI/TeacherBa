# -*- coding: utf-8 -*-
"""TeacherBa_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ux_B-C36UjCh2h0E6wKQfBlWbYR_8yx7
"""

!pip install python-dotenv
!pip install langchain-core
!pip install langchain_openai

from dotenv import load_dotenv
load_dotenv()

import os
from glob import glob

from pprint import pprint
import json

from langchain_core.tools import tool

from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command
from langchain_openai import AzureChatOpenAI

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import AzureChatOpenAI

from IPython.display import Image, display

"""# State"""

# 상태
class State(TypedDict):   # TypedDict 클래스에서 상속받아 정의, State: 각 단계별로 정도 넘김
    input_message: str    # query
    final_answer: str     # 최종 답변

"""# Node and Edge"""

# LLM 인스턴스 생성
#llm = AzureChatOpenAI(model="gpt-4o-mini")

# Node

# Relevance
def relevance():

def traffic_theme(state:State) -> Command[Literal["",""]]:

def traffic_law(state: State) -> Command[Literal[END]]:
  """교통 및 운전 관련 법과 규칙에 관한 노드"""

  # 저장된 벡터 저장소를 가져오기
  chroma_db = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings_model
  )

  # mmr 검색기 생성
  retriever = chroma_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k":10,"lambda_mult": 0.4},
  )

  # template
  template = """주어진 컨텍스트를 기반으로 질문에 답변하시오.

  [지침]
  - 컨텍스트에 있는 정보만을 사용하여 답변할 것
  - 불확실한 경우 명확히 그 불확실성을 표현할 것
  - 답변은 논리적이고 구조화된 형태로 제공할 것
  - 답변은 한국어를 사용할 것

  [컨텍스트]
  {context}

  [질문]
  {question}

  [답변]
  """

  prompt = ChatPromptTemplate.from_template(template)

  # 문서 포맷팅
  def format_docs(docs):
      return "\n\n".join([f"{doc.page_content}" for doc in docs])

  # RAG 체인 생성
  rag_chain = (
      RunnableParallel(
          {
              "context": retriever | format_docs,
              "question": RunnablePassthrough()
          }
      )
      | prompt
      | llm
      | StrOutputParser()
  )

  # 체인 실행
  query = state["input_message"]
  output = rag_chain.invoke(query)

  # 상태 업데이트와 함께 다음 노드로 라우팅
  return Command(
      goto=END,
      update={"final_answer": output}
  )

def car_maintanance(state: State) -> Command[Literal[END]]:











from langchain_community.tools import TavilySearchResults

def traffic_general(state: State) -> Command[Literal[END]]:
  """교통 법령 및 차량 정비 외의 교통에 관한 노드"""

  # define tool
  tool = TavilySearchResults(
      max_result=3,
      search_depth="advanced",
      include_answer=True,
      include_raw_content=True,
      include_images=True
  )

  # template
  template = """주어진 컨텍스트를 기반으로 질문에 답변하시오.

  [지침]
  - 컨텍스트에 있는 정보만을 사용하여 답변할 것
  - 불확실한 경우 명확히 그 불확실성을 표현할 것
  - 답변은 논리적이고 구조화된 형태로 제공할 것
  - 답변은 한국어를 사용할 것

  [컨텍스트]
  {context}

  [질문]
  {question}

  [답변]
  """

  prompt = ChatPromptTemplate.from_template(template)\

  # 문서 포맷팅
  def format_docs(docs):
      return "\n\n".join([f"{doc.page_content}" for doc in docs])

  # RAG 체인 생성
  rag_chain = (
      RunnableParallel(
          {
              "context": tool | format_docs,
              "question": RunnablePassthrough()
          }
      )
      | prompt
      | llm
      | StrOutputParser()
  )

  # 체인 실행
  query = state["input_message"]
  output = rag_chain.invoke(query)

  # 상태 업데이트와 함께 다음 노드로 라우팅
  return Command(
      goto=END,
      update={"final_answer": output}
  )

# 요약 생성 노드
def generate_summary(state: State) -> Command[Literal["improve_summary", "finalize_summary"]]:
    """원본 텍스트를 요약하고 품질을 평가하는 노드"""
    # 요약 생성
    summary_prompt = f"""다음 텍스트를 핵심 내용 중심으로 간단히 요약해주세요:

    [텍스트]
    {state['original_text']}

    [요약]
    """
    summary = summary_llm.invoke(summary_prompt).content

#     summary = """인공지능은 컴퓨터 과학의 한 분야이며 인간의 능력을 구현한 것인데 \
# 요즘에는 정말 다양한 분야에서 활용되고 있고 특히 기계학습이랑 딥러닝이 발전하면서 \
# 더욱 활용도가 높아지고 있다고 합니다"""  # 테스트용

    # 품질 평가
    eval_prompt = f"""다음 요약의 품질을 평가해주세요.
    요약이 명확하고 핵심을 잘 전달하면 'good'을,
    개선이 필요하면 'needs_improvement'를 응답해주세요.

    요약본: {summary}
    """
    quality = eval_llm.invoke(eval_prompt).content.lower().strip()

    # 상태 업데이트와 함께 다음 노드로 라우팅
    return Command(
        goto="finalize_summary" if "good" in quality else "improve_summary",
        update={"summary": summary}
    )

# 요약 개선 노드
def improve_summary(state: State) -> Command[Literal[END]]:
    """요약을 개선하고 다듬는 노드"""
    prompt = f"""다음 요약을 더 명확하고 간결하게 개선해주세요:

    [기존 요약]
    {state['summary']}

    [개선 요약]
    """
    improved_summary = llm.invoke(prompt).content

    # 상태 업데이트와 함께 다음 노드로 라우팅
    return Command(
        goto=END,
        update={"final_summary": improved_summary}
    )

# 최종 요약 설정 노드
def finalize_summary(state: State) -> Command[Literal[END]]:
    """현재 요약을 최종 요약으로 설정하는 노드"""

    # 상태 업데이트와 함께 다음 노드로 라우팅
    return Command(
        goto=END,
        update={"final_summary": state["summary"]}
    )

"""# Graph"""

!pip install langgraph
!pip install typing

from langgraph.graph import StateGraph, START, END
from IPython.display import Image, display

# 워크플로우 구성
workflow = StateGraph(State)

# 노드 추가
workflow.add_node("identify_query", identify_query)
workflow.add_node("identify_low_or_maintenance", identify_low_or_maintenance)
workflow.add_node("traffic_law", traffic_law)
workflow.add_node("car_maintanance", car_maintanance)
workflow.add_node("traffic_general", traffic_general)

# 기본 엣지 추가
workflow.add_edge(START, "identify_query")

# 그래프 컴파일
graph = workflow.compile()

# 그래프 시각화
display(Image(graph.get_graph().draw_mermaid_png()))

# Implement
initial_state = {
    "input_message": text, # text는 input query로 대체
}

for chunk in graph.stream(initial_state, stream_mode="values"):
    pprint(chunk)
    print("=" * 100)

!python --version

|