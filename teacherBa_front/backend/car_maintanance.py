# -*- coding: utf-8 -*-
"""Car_Maintanance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WV8k0cpDj76aBLWaKjScSJ2joe81FVcj

# 차량정비
- RAG(PDF)
- LLM
"""

# !pip install openai
# !pip install python-dotenv
# !pip install langchain-openai
# !pip install langchain_community
# !pip install pypdf
# !pip install chromadb
# !pip install langchain_chroma
# !pip install tavily-python langchain-community langchain-openai

import os
import openai
from IPython.display import Markdown, display

from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings

from langchain_community.utils.math import cosine_similarity
from langchain_core.prompts import ChatPromptTemplate
import numpy as np
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

from langchain_community.tools import TavilySearchResults

load_dotenv()

from langchain_community.document_loaders import PyPDFLoader

# PDF 로더 초기화 (근로기준법 문서)
pdf_loader_1 = PyPDFLoader("/content/Car_maintenance.pdf")

# 동기 로딩
pdf_docs_1 = pdf_loader_1.load()

print("문서의 수 :", len(pdf_docs_1))
print("-" * 100)
print("처음 문서의 메타데이터 :", pdf_docs_1[0].metadata)

"""### Text splitter - pdf_docs_1"""

# 재귀적 텍스트 분할기 초기화
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,             # 청크 크기
    chunk_overlap=200,           # 청크 중 중복되는 부분 크기
    length_function=len,         # 글자 수를 기준으로 분할
    separators=["\n\n", "\n", " ", ""],  # 구분자 - 재귀적으로 순차적으로 적용 [문단, 문장, 띄어쓰기, 공백]
)

# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할
chunks = text_splitter.split_documents(pdf_docs_1)
print(f"생성된 텍스트 청크 수: {len(chunks)}")
print(f"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}")
print()

"""### Text splitter - pdf_docs_2"""

embeddings_model = AzureOpenAIEmbeddings(model="text-embedding-3-small")

document_embeddings_openai = embeddings_model.embed_documents([doc.page_content for doc in pdf_docs_1])
print(f"임베딩 백터의 개수: {len(document_embeddings_openai)}")
print(f"각 벡터의 차원: {len(document_embeddings_openai[0])}")

len(document_embeddings_openai[0])

q = embeddings_model.embed_query("차에서 이상한 냄새나 연기가 보이면 어떻게 점검해야 해?")

# 쿼리와 가장 유사한 문서 찾기 함수
def find_most_similar(
        query: str,
        doc_embeddings: np.ndarray,
        embeddings_model=AzureOpenAIEmbeddings(model="text-embedding-3-small") # Removed dimensions parameter
        ) -> tuple[str, float]:

    # 쿼리 임베딩: OpenAI 임베딩 사용
    query_embedding = embeddings_model.embed_query(query)

    # 코사인 유사도 계산
    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]

    # 가장 유사한 문서 인덱스 찾기
    most_similar_idx = np.argmax(similarities)

    # 가장 유사한 문서와 유사도 반환: 문서, 유사도
    return pdf_docs_1[most_similar_idx].page_content, similarities[most_similar_idx] # Assuming pdf_docs_1 is the correct document list

# 예제 쿼리
queries = [
    "차에서 이상한 냄새나 연기가 보이면 어떻게 점검해야 해?"
]

# 각 쿼리에 대해 가장 유사한 문서 찾기
for query in queries:
    most_similar_doc, similarity = find_most_similar(query, document_embeddings_openai)
    print(f"쿼리: {query}")
    print(f"가장 유사한 문서: {most_similar_doc}")
    print(f"유사도: {similarity}")
    print("-" * 100)

from langchain_community.document_loaders import PyPDFLoader

# PDF 로더 초기화
pdf_loader = PyPDFLoader("/content/Car_maintenance.pdf")

# 동기 로딩
pdf_docs = pdf_loader.load()
print("문서의 페이지 수:", len(pdf_docs))

from langchain_text_splitters import RecursiveCharacterTextSplitter

# TikToken 인코더를 사용하여 재귀적 텍스트 분할기 초기화 (토큰 수 기준 분할)
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base",
    chunk_size=500,
    chunk_overlap=100,
)

# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할
chunks = text_splitter.split_documents(pdf_docs)

print(f"생성된 청크 수: {len(chunks)}")
print(f"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}")

# 각 청크의 시작 부분과 끝 부분 확인
for chunk in chunks[:3]:
    print(chunk.page_content[:100] + "---" + chunk.page_content[-100:])
    print("-" * 100)

from langchain_chroma import Chroma
from langchain_openai import AzureOpenAIEmbeddings

# OpenAI 임베딩 모델 생성
embedding_model = AzureOpenAIEmbeddings(model = "text-embedding-3-small")  # 차원 지정 안 하면 1500차원 됨

# Chroma 벡터 저장소 생성하기
chroma_db = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory="./chroma_db"
)

# 문서 개수 확인 (벡터db에 저장이 된 상태)
chroma_db._collection.count()

from langchain_chroma import Chroma

# 저장된 벡터 저장소를 가져오기
chroma_db = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embedding_model
)


results = chroma_db.similarity_search_with_score(
                  query,
                  k=1,                    # 1개만 뽑아줘
                  filter={"source":"/content/Car_maintenance.pdf"}   # 특정 pdf 지정
)

results[0]

#prompt
template = """주어진 컨텍스트를 기반으로 질문에 답변하시오.

[지침]
- 컨텍스트에 있는 정보만을 사용하여 답변할 것
- 외부 지식이나 정보를 사용하지 말 것
- 컨텍스트에서 답을 찾을 수 없는 경우 "주어진 정보만으로는 답변하기 어렵습니다."라고 응답할 것
- 불확실한 경우 명확히 그 불확실성을 표현할 것
- 답변은 논리적이고 구조화된 형태로 제공할 것
- 답변은 한국어를 사용할 것

[컨텍스트]
{context}

[질문]
{question}

[답변 형식]
1. 핵심 답변: (질문에 대한 직접적인 답변)
2. 추가 설명: (필요한 경우 부연 설명 제공)

[답변]
"""

prompt = ChatPromptTemplate.from_template(template)

# 템플릿 출력
prompt.pretty_print()

#model
llm = AzureChatOpenAI(
    deployment_name="gpt-4o-mini",
    temperature=0.2)

# Travily


tool = TavilySearchResults(
        max_results=5,              # 최대 5개까지 반환 가능
        search_depth="advanced",    # basic: 표면적으로만 검색하고 끝, advanced: 좀 더 깊숙히 검색
        include_answer=True,        # 결과에 직접적인 영향
        include_raw_content=True,   # 정제되지 않는 원천소스(row_content) 리턴
        include_images=True,
        )


from pprint import pprint

def summarize_search_results(query, tool, llm):
    """
    Performs a search using the provided tool, extracts the content,
    and summarizes it using the provided LLM.

    Args:
        query (str): The search query.
        tool: The search tool to use (e.g., TavilySearchResults).
        llm: The language model to use for summarization.

    Returns:
        str: The summarized content, or a message indicating no content was found.
    """
    result = tool.invoke(query)

    # Extract content from the search results
    search_content = ""
    if result:
        for item in result:
            if 'content' in item:
                search_content += item['content'] + "\n\n"

    # Summarize the content using the existing LLM
    if search_content:
        summary_query = f"다음 내용을 요약해 주세요:\n\n{search_content}"
        summary_output = llm.invoke(summary_query)
        return summary_output.content  # Accessing content attribute for the response object
    else:
        return "검색 결과에서 내용을 찾을 수 없습니다."

#chain
retriever = chroma_db.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5,
                   "fetch_k": 10,
                   "lambda_mult": 0.3},
)


# 문서 포맷팅
def format_docs(docs):
    return "\n\n".join([f"{doc.page_content}" for doc in docs])

# RAG 체인 생성
rag_chain = (
    RunnableParallel(
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
    )
    | prompt
    | llm
    | StrOutputParser()
)

# 체인 실행
query = "차에서 이상한 냄새나 나거나 연기가 보이면 어떻게 점검해야 해?"
output = rag_chain.invoke(query)



print(f"쿼리: {query}")
print("답변:")
print(output)