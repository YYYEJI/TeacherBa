# -*- coding: utf-8 -*-
"""TeacherBa_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XzYDtdp5rif6P2G6GGuEF9XezfyIZb4V

# ðŸŒŸ ì´ˆë³´ìš´ì „ ë„ìš°ë¯¸ í”„ë¡œì íŠ¸ - ë°”ì„ ìƒ ðŸŒŸ

- êµí†µì— ëŒ€í•œ ì‚¬ìš©ìž ì§ˆë¬¸ì„ ë°›ì•„ ë¬¸ì œë¥¼ ë¶„ë¥˜í•˜ê³ ,
ê·¸ì— ë§žëŠ” ì§„ë‹¨ ë° í•´ê²° ë°©ë²•ì„ ì œê³µí•˜ëŠ” ì›¹ ê¸°ë°˜ í•™ìŠµ ë„ìš°ë¯¸ìž…ë‹ˆë‹¤.

###  âš’ï¸ ì£¼ìš” ê¸°ëŠ¥

- ì‚¬ìš©ìž ì§ˆë¬¸ ìž…ë ¥ ê¸°ëŠ¥ (ì˜ˆ: "ë¸Œë ˆì´í¬ê°€ ì´ìƒí•´ìš”", "ì°¨ê°€ ì‹œë™ì´ ì•ˆ ê±¸ë ¤ìš”")
- ê° ë¬¸ì œì— ëŒ€í•œ ì›ì¸ ë¶„ì„ ë° ê°œì¸ì´ ì·¨í•  ìˆ˜ ìžˆëŠ” ì¡°ì¹˜ ì œì•ˆ
- HTMLì„ ì´ìš©í•œ ê°„ë‹¨í•œ ì‚¬ìš©ìž ì¸í„°íŽ˜ì´ìŠ¤ ì œê³µ
"""

!pip install python-dotenv
!pip install langchain-core
!pip install langchain_openai
!pip install langgraph
!pip install typing
!pip install openai
!pip install langchain_community
!pip install pypdf
!pip install langchain_chroma

from dotenv import load_dotenv

load_dotenv()

import os
from glob import glob

from pprint import pprint
import json

from langchain_core.tools import tool
from langchain_core.tools import tool
from typing import Literal
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from langchain_openai import AzureChatOpenAI
llm = AzureChatOpenAI(model="gpt-4.1",temperature=0)

from langchain.agents import create_tool_calling_agent
from langchain.agents import AgentExecutor

from typing import TypedDict
from langchain_openai import AzureChatOpenAI

from langgraph.graph import StateGraph, START, END
from IPython.display import Image, display
from typing import Literal

from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command
from langchain_openai import AzureChatOpenAI

from langchain_community.tools import TavilySearchResults

import openai
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.utils.math import cosine_similarity
from langchain_core.runnables import RunnableParallel
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import glob
from langchain_core.runnables import RunnableLambda



# ìƒíƒœ
class State(TypedDict):   # TypedDict í´ëž˜ìŠ¤ì—ì„œ ìƒì†ë°›ì•„ ì •ì˜, State: ê° ë‹¨ê³„ë³„ë¡œ ì •ë„ ë„˜ê¹€
    input_message: str    # query
    final_answer: str     # ìµœì¢… ë‹µë³€

"""# RAG ì •ì˜

"""

# Get list of all PDF files matching the pattern
pdf_files = glob.glob("/content/data/*.pdf")

pdf_docs = []
for pdf_file in pdf_files:
    loader = PyPDFLoader(pdf_file)
    docs = loader.load()
    pdf_docs.extend(docs)

    from langchain_community.document_loaders import WebBaseLoader

# ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
web_loader = WebBaseLoader(
    web_path = [
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=1&cciNo=1&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=1&cciNo=1&cnpClsNo=2&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=1&cciNo=2&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=1&cciNo=2&cnpClsNo=2&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=2&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=3&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=4&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=5&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=6&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=2&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=3&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=4&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=5&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=6&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=2&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=3&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=4&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=5&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=1&cnpClsNo=6&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=2&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=3&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=2&cciNo=3&cnpClsNo=2&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=3&cciNo=1&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=3&cciNo=1&cnpClsNo=2&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=3&cciNo=1&cnpClsNo=3&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=4&cciNo=1&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=4&cciNo=1&cnpClsNo=2&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=4&cciNo=2&cnpClsNo=1&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=4&cciNo=2&cnpClsNo=2&search_put=",
        "https://www.easylaw.go.kr/CSP/CnpClsMain.laf?popMenu=ov&csmSeq=684&ccfNo=4&cciNo=2&cnpClsNo=3&search_put="

    ]
)

# ë™ê¸° ë¡œë”©
web_docs = web_loader.load()

# 3. í†µí•© ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
all_docs = pdf_docs + web_docs

print("PDF ë¬¸ì„œ ìˆ˜:", len(pdf_docs))
print("ì›¹ ë¬¸ì„œ ìˆ˜:", len(web_docs))
print("ì „ì²´ ë¬¸ì„œ ìˆ˜:", len(all_docs))
print("ì²« ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:", all_docs[0].metadata)

###############
# Text Splitting
################

# ìž¬ê·€ì  í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,             # ì²­í¬ í¬ê¸°
    chunk_overlap=200,           # ì²­í¬ ì¤‘ ì¤‘ë³µë˜ëŠ” ë¶€ë¶„ í¬ê¸°
    length_function=len,         # ê¸€ìž ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    separators=["\n\n", "\n", " ", ""],  # êµ¬ë¶„ìž - ìž¬ê·€ì ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©
)

# split_documents() ë©”ì„œë“œ ì‚¬ìš© : Document ê°ì²´ë¥¼ ì—¬ëŸ¬ ê°œì˜ ìž‘ì€ ì²­í¬ ë¬¸ì„œë¡œ ë¶„í• 
all_docs = pdf_docs + web_docs # ë‘ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°
chunks = text_splitter.split_documents(all_docs) # ì „ì²´ ë¬¸ì„œ ë¶„í• 
print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜: {len(chunks)}")
print(f"ê° ì²­í¬ì˜ ê¸¸ì´: {list(len(chunk.page_content) for chunk in chunks)}")
print()

# Check Chunks
print(chunks[3].page_content)
print("=" * 50)
print(chunks[200].page_content)
print("=" * 50)
print(chunks[20].page_content)
print("=" * 50)
print(chunks[21].page_content)

###############
# Embedding
###############


# OpenAIEmbeddings ëª¨ë¸ ìƒì„±
embeddings_model = AzureOpenAIEmbeddings(model="text-embedding-3-small", dimensions=1024)

# ìž„ë² ë”© ê°ì²´ ì¶œë ¥
embeddings_model

# ë¬¸ì„œ ìž„ë² ë”©
texts = [doc.page_content for doc in chunks]
document_embeddings_openai = embeddings_model.embed_documents(texts)

# ìž„ë² ë”© ê²°ê³¼ ì¶œë ¥
print(f"ìž„ë² ë”© ë²¡í„°ì˜ ê°œìˆ˜: {len(document_embeddings_openai)}")
print(f"ê° ë²¡í„°ì˜ ì°¨ì›: {len(document_embeddings_openai[0])}")

# Chroma ë²¡í„° ì €ìž¥ì†Œ ìƒì„±í•˜ê¸°
chroma_db = Chroma.from_documents(
    documents = chunks,
    embedding = embeddings_model,
    persist_directory="./db_traffic" # db ì €ìž¥
)

# ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
print("ë¬¸ì„œ ìˆ˜:", len(chroma_db.get()["documents"]))  # âœ… ì•ˆì „í•œ ë°©ì‹

"""# Identify_query (A)
- ë°›ì€ ì§ˆë¬¸ì´ ìš´ì „ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ ì•„ë‹Œì§€ íŒŒì•…í•˜ëŠ” ë…¸ë“œ
"""

def identify_query(state: State) -> Command[Literal["identify_low_or_maintenance", "no_traffic"]]:
    """ë°›ì€ ì§ˆë¬¸ì´ ìš´ì „ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ ì•„ë‹Œì§€ íŒŒì•…í•˜ëŠ” ë…¸ë“œ"""


    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì´ ìš´ì „, êµí†µ, ìžë™ì°¨ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ì§€ íŒŒì•…í•´ì£¼ì„¸ìš”:

    [ì§ˆë¬¸]
    {state['input_message']}

    [ë‹µë³€]
    yes í˜¹ì€ noë¡œ ë‹µë³€í•´ì¤˜.
    """

    response = llm.invoke(prompt)
    answer = response.content.lower().strip()

    # return {"Identify": response.content}
    return Command(
        goto="identify_low_or_maintenance" if "yes" in answer else "no_traffic",
    )

def no_traffic(state: State) -> Command[Literal[END]]:
  """êµí†µê³¼ ê´€ë ¨ë˜ì§€ ì•Šì€ ì§ˆë¬¸ì— ëŒ€í•œ ë…¸ë“œ"""

  return Command(
      goto=END,
      update={"final_answer": "êµí†µê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ë§Œ í•´ì£¼ì„¸ìš”."}
  )

"""# Identify_Law_or_Maintenance (B)
- ë°›ì€ ì§ˆë¬¸ì´ êµí†µë²•ê·œ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ ì°¨ëŸ‰ì •ë¹„ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ íŒŒì•…
"""

def identify_low_or_maintenance(state: State)-> Command[Literal["traffic_law", "traffic_general"]]:
    """ë°›ì€ ì§ˆë¬¸ì´ ìš´ì „ë²•ê·œ/ì°¨ëŸ‰ì •ë¹„ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ ì•„ë‹ˆë©´ ìš´ì „ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ íŒŒì•…í•˜ëŠ” ë…¸ë“œ"""


    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì´ ìš´ì „ë²•ê·œ/ì°¨ëŸ‰ì •ë¹„ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ ì•„ë‹Œì§€ íŒŒì•…í•´ì£¼ì„¸ìš”:

    [ì§ˆë¬¸]
    {state['input_message']}

    [ë‹µë³€]
    yes í˜¹ì€ noë¡œ ë‹µë³€í•´ì¤˜.

    """

    response = llm.invoke(prompt)
    answer = response.content.lower().strip()

    return Command(
        goto="traffic_law" if "yes" in answer else "traffic_general")
        # update={"summary": summary}

"""# Traffic_Law (C)
- ì°¨ëŸ‰ë²•ê·œ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€
- RAG, LLM
"""

def traffic_law(state: State) -> Command[Literal[END]]:
    """êµí†µ ë° ìš´ì „ ê´€ë ¨ ë²•ê³¼ ê·œì¹™ì— ê´€í•œ ë…¸ë“œ"""
    # OpenAIEmbeddings ëª¨ë¸ ìƒì„±
    embeddings_model = AzureOpenAIEmbeddings(model="text-embedding-3-small", dimensions=1024)

    # ì €ìž¥ëœ ë²¡í„° ì €ìž¥ì†Œë¥¼ ê°€ì ¸ì˜¤ê¸°
    chroma_db = Chroma(
      persist_directory="./db_traffic",
      embedding_function=embeddings_model
    )

    # mmr ê²€ìƒ‰ê¸° ìƒì„±
    retriever = chroma_db.as_retriever(
      search_type="mmr",
      search_kwargs={"k": 4, "fetch_k":10,"lambda_mult": 0.4},
    )

    # template
    template = """ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹œì˜¤.

    [ì§€ì¹¨]
    - ì»¨í…ìŠ¤íŠ¸ì— ìžˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•  ê²ƒ
    - ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•ížˆ ê·¸ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•  ê²ƒ
    - ë‹µë³€ì€ ë…¼ë¦¬ì ì´ê³  êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì œê³µí•  ê²ƒ
    - ë‹µë³€ì€ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•  ê²ƒ

    [ì»¨í…ìŠ¤íŠ¸]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """

    prompt = ChatPromptTemplate.from_template(template)

    # ë¬¸ì„œ í¬ë§·íŒ…
    def format_docs(docs):
        return "\n\n".join([f"{doc.page_content}" for doc in docs])

    # RAG ì²´ì¸ ìƒì„±
    rag_chain = (
        RunnableParallel(
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough()
            }
        )
        | prompt
        | llm
        | StrOutputParser()
    )

    # ì²´ì¸ ì‹¤í–‰
    query = state["input_message"]
    output = rag_chain.invoke(query)

    # ìƒíƒœ ì—…ë°ì´íŠ¸ì™€ í•¨ê»˜ ë‹¤ìŒ ë…¸ë“œë¡œ ë¼ìš°íŒ…
    return Command(
        goto=END,
        update={"final_answer": output}
    )

"""# traffic_general (E)
- ì´ì™¸ì˜ êµí†µ ê´€ë ¨ ì‚¬í•­ ë‹µë³€
- Web search, LLM
"""

def traffic_general(state: State) -> Command[Literal[END]]:
  """êµí†µ ë²•ë ¹ ë° ì°¨ëŸ‰ ì •ë¹„ ì™¸ì˜ êµí†µì— ê´€í•œ ë…¸ë“œ"""

  # define tool
  tool = TavilySearchResults(
      max_result=3,
      search_depth="advanced",
      include_answer=True,
      include_raw_content=True,
      include_images=True
  )


  result = tool.invoke(state["input_message"])

  # template
  template = """ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹œì˜¤.

  [ì§€ì¹¨]
  - ì»¨í…ìŠ¤íŠ¸ì— ìžˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•  ê²ƒ
  - ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•ížˆ ê·¸ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•  ê²ƒ
  - ë‹µë³€ì€ ë…¼ë¦¬ì ì´ê³  êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì œê³µí•  ê²ƒ
  - ë‹µë³€ì€ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•  ê²ƒ

  [ì»¨í…ìŠ¤íŠ¸]
  {context}

  [ì§ˆë¬¸]
  {question}

  [ë‹µë³€]
  """


  prompt = ChatPromptTemplate.from_template(template)

  # RAG ì²´ì¸ ìƒì„±
  rag_chain = (
      RunnableParallel(
          {
              "context": tool,
              "question": RunnablePassthrough()
          }
      )
      | prompt
      | llm
      | StrOutputParser()
  )


  # ì²´ì¸ ì‹¤í–‰
  query = state["input_message"]
  output = rag_chain.invoke(query)

  # ìƒíƒœ ì—…ë°ì´íŠ¸ì™€ í•¨ê»˜ ë‹¤ìŒ ë…¸ë“œë¡œ ë¼ìš°íŒ…
  return Command(
      goto=END,
      update={"final_answer": output}
  )

"""# Workflow"""

# ì›Œí¬í”Œë¡œìš° êµ¬ì„±
workflow = StateGraph(State)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("identify_query", identify_query)
workflow.add_node("identify_low_or_maintenance", identify_low_or_maintenance)
workflow.add_node("no_traffic", no_traffic)
workflow.add_node("traffic_general", traffic_general)
workflow.add_node("traffic_law", traffic_law)

# ê¸°ë³¸ ì—£ì§€ ì¶”ê°€
workflow.add_edge(START, "identify_query")

# ê·¸ëž˜í”„ ì»´íŒŒì¼
graph = workflow.compile()

# ê·¸ëž˜í”„ ì‹œê°í™”
display(Image(graph.get_graph().draw_mermaid_png()))

# text = "í™”ë¬¼ìžë™ì°¨ì˜ ì ìž¬ ì œí•œì— ëŒ€í•´ ì•Œë ¤ì¤˜"
#text = "ê³„ê¸°íŒì— ëŠë‚Œí‘œ ê²½ê³ ë“±ì´ ë–´ëŠ”ë° ë¬´ìŠ¨ ëœ»ì´ì—ìš”?"
#text = "ìžë™ì°¨ ìœ ë¦¬ì„¸ì •ì œë¥¼ ì–´ë””ì„œ êµ¬ë§¤í•  ìˆ˜ ìžˆì„ê¹Œ?"
#text = "ì˜¤ëŠ˜ ì ì‹¬ ë­ ë¨¹ì„ê¹Œ?"


# Implement
initial_state = {
    "input_message": text, # textëŠ” input queryë¡œ ëŒ€ì²´
}

from IPython.display import Markdown, display
from pprint import pprint

for chunk in graph.stream(initial_state, stream_mode="values"):
#    pprint(chunk)   # í•„ìš”í•˜ë©´ ì¶œë ¥
#    print("=" * 100)

    final_answer = chunk.get('final_answer')
    if final_answer:
        display(Markdown(final_answer))

# íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸
#!pip freeze > requirements.txt

